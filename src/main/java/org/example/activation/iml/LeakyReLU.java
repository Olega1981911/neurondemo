package org.example.activation.iml;

import org.example.activation.IActivationFunction;

//LeakyReLU - это вариант функции ReLU, который позволяет небольшим отрицательным значениям, когда x < 0.
// Это делается для того, чтобы избежать проблемы “мертвых нейронов”,
// которая может возникнуть при использовании стандартной функции ReLU.
public class LeakyReLU implements IActivationFunction {

    // output(double x): Этот метод вычисляет значение функции активации для заданного входа x.
    // Если x >= 0, он возвращает x; в противном случае он возвращает x * 0.01.
    @Override
    public double output(double x) {
        return x >= 0 ? x : x * 0.01;
    }

    // outputDerivative(double x): Этот метод вычисляет производную функции активации для заданного входа x.
    // Если x >= 0, он возвращает 1; в противном случае он возвращает 0.01.
    @Override
    public double outputDerivative(double x) {
        return x >= 0 ? 1 : 0.01;
    }

    //Эти два метода важны для обучения нейронной сети, поскольку они используются при прямом и обратном распространении ошибки.
    // Производная функции активации используется при обратном распространении ошибки для обновления весов и смещений нейронов.
}
